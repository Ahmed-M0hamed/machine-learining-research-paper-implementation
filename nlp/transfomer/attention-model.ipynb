{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled46.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NMT attention based model \n",
        "most of the models which were known before this paper were based on the encoder and decoder family they were good but they had some problems \n",
        "\n",
        "- they had to encode the whole sentence in just one fixed encoder vector so the decoder had to output the whole target just from this vector so these models strugled with the long sentences \n",
        "\n",
        "so in this paper the authors proposed to use attention approch to overcome this drawback so what they had done ? \n"
      ],
      "metadata": {
        "id": "rgJVQ04UERrI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_D1-03_JDwxt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import matplotlib.ticker as ticker\n",
        "import string\n",
        "from string import digits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## we will use english to spanish dataset "
      ],
      "metadata": {
        "id": "BmG92HxYF9mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxrBaGJPD54z",
        "outputId": "042626e2-bd50-49cc-d9c2-376b3979da57"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n",
            "2654208/2638744 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## processing the sentences \n",
        "here we process the sentence by removing the qoutes and elminating the punctions and add the start and end tocken to the sentence "
      ],
      "metadata": {
        "id": "H7Djrko4GJZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    \n",
        "    num_digits= str.maketrans('','', digits)\n",
        "    \n",
        "    sentence= sentence.lower()\n",
        "    sentence= re.sub(\" +\", \" \", sentence)\n",
        "    sentence= re.sub(\"'\", '', sentence)\n",
        "    sentence= sentence.translate(num_digits)\n",
        "    sentence= re.sub(r\"([?.!,Â¿])\", r\" \\1 \", sentence)\n",
        "    sentence = sentence.rstrip().strip()\n",
        "    sentence=  'start_ ' + sentence + ' _end'\n",
        "    \n",
        "    return sentence"
      ],
      "metadata": {
        "id": "PxPr9Fa5nLRE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load the data \n",
        "we load the data and listed it while processing each sentence "
      ],
      "metadata": {
        "id": "_Gr9Rcj7GhaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "  text = path.read_text(encoding='utf-8')\n",
        "\n",
        "  lines = text.splitlines()\n",
        "  pairs = [line.split('\\t') for line in lines]\n",
        "\n",
        "  inp = [preprocess_sentence(inp) for targ, inp in pairs]\n",
        "  targ = [preprocess_sentence(targ) for targ, inp in pairs]\n",
        "\n",
        "  return targ, inp"
      ],
      "metadata": {
        "id": "78JNz6VKEmkb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targ,inp  = load_data(path_to_file)"
      ],
      "metadata": {
        "id": "0kG7ov3uTBnz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## process the tockens \n",
        "we tokenize the inputs and targets sentences \n",
        "and we zero padded them to align the whole inputs and targets "
      ],
      "metadata": {
        "id": "tyjlJNu2GxrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "# Fit the source sentences to the source tokenizer\n",
        "input_sentence_tokenizer.fit_on_texts(inp)\n",
        "input_tensor = input_sentence_tokenizer.texts_to_sequences(inp)\n",
        "input_tensor= tf.keras.preprocessing.sequence.pad_sequences(input_tensor,padding='post' )"
      ],
      "metadata": {
        "id": "VW-kisCSUUrm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "# Fit the source sentences to the source tokenizer\n",
        "target_sentence_tokenizer.fit_on_texts(targ)\n",
        "target_tensor = target_sentence_tokenizer.texts_to_sequences(targ)\n",
        "target_tensor= tf.keras.preprocessing.sequence.pad_sequences(target_tensor,padding='post' )"
      ],
      "metadata": {
        "id": "r4n_MoefUUWP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE  = len(inp)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor , target_tensor)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "XYXbOYufTE4P"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_batch, target_batch =next(iter(dataset))"
      ],
      "metadata": {
        "id": "Sz_MfJd6VvwX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UmhaWqa_jhJ",
        "outputId": "2086035f-80a6-4fc2-dc17-de0eeef04291"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 53])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "steps_per_epoch= len(input_tensor)//BATCH_SIZE\n",
        "embedding_dim=256\n",
        "units=1024\n",
        "input_vocab_size= len(input_sentence_tokenizer.word_index)+1\n",
        "target_vocab_size= len(target_sentence_tokenizer.word_index)+1"
      ],
      "metadata": {
        "id": "exM6IC5eTrJ5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## the Encoder \n",
        "the Encoder start with embedding layer \n",
        "after that we pass the embedded inputs throw GRU layer with return sequence prameter to True to return the wohle sequnce not just the last output vector and return_state prameter to True  \n",
        "\n",
        "the mehtod initialize_hidden_state just initialize the first hidden state to zeros \n"
      ],
      "metadata": {
        "id": "3nMidHxgHIwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model) : \n",
        "  def __init__(self ,embedding_dim ,units , vocab_size  ,batch_size): \n",
        "    self.embedding_dim =embedding_dim\n",
        "    self.units = units \n",
        "    self.vocab_size = vocab_size \n",
        "    self.batch_size = batch_size\n",
        "    super(Encoder ,self).__init__()\n",
        "    self.encoder_embedding = tf.keras.layers.Embedding(vocab_size , embedding_dim ) \n",
        "    self.encode_gru = tf.keras.layers.GRU(units , return_sequences=True ,return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "  def call(self, input_tokens , state ) : \n",
        "    embedding_vectors = self.encoder_embedding(input_tokens) \n",
        "    output , state = self.encode_gru(embedding_vectors , initial_state = state ) \n",
        "    return output , state\n",
        "  def initialize_hidden_state(self  ):\n",
        "    return  tf.zeros((self.batch_size, self.units))"
      ],
      "metadata": {
        "id": "g_yxNvZxT84_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder( embedding_dim, units, input_vocab_size,BATCH_SIZE)\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden= encoder(source_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "metadata": {
        "id": "gAE3OeaV_RWo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## the attention block \n",
        "this class explain attnetion in details \n",
        "\n",
        "to apply attention we need for each time stamp of the decoder we need context vector and to calculate this context vector we need attention weights and to calculate the attention weights we need the score function so we will explain each one in detail \n",
        "\n",
        "### the attention weights  \n",
        "we know that attention weights we will get tell us how mush attention we could pay to each output of the encoder throw spacific time stamp of the decoder \n",
        "\n",
        "attention_weights = softmax(score_function) \n",
        "\n",
        "### the score function \n",
        "but to initialize this attention weights we have to give her some Intuitions about the last time stamp of the decoder to be able to weight the right parts of the encoder \n",
        "socre = dense(tanh(dense(last_time_stamp) + (encoder_outputs) )) \n",
        "\n",
        "### the context vector \n",
        "the context = sum( attention_weights * encoder_outputs ) \n",
        "\n"
      ],
      "metadata": {
        "id": "cr6XFjuRI_lP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer): \n",
        "  def __init__(self, units ): \n",
        "    super(BahdanauAttention , self).__init__()\n",
        "    self.units = units\n",
        "    self.encoder_output_dense = tf.keras.layers.Dense(units )\n",
        "    self.decoder_pre_state_dense = tf.keras.layers.Dense(units)\n",
        "    self.init_attention_weights_dense  = tf.keras.layers.Dense(1) \n",
        "  def call(self , query, values) : \n",
        "    score = self.init_attention_weights_dense(tf.nn.tanh(self.encoder_output_dense(values) + self.decoder_pre_state_dense(tf.expand_dims(query , 1))))\n",
        "    attention_weights  = tf.nn.softmax(score ,axis =1)  \n",
        "    context = tf.math.reduce_sum(attention_weights * values , axis = 1) \n",
        "    return context , attention_weights"
      ],
      "metadata": {
        "id": "td3ggm6NY5b2"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer= BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep2NujFn_84d",
        "outputId": "9956ce06-e76a-4927-d009-101ae73d82b1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 53, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder \n",
        "the decoder includes embedding layer for the target sentences \n",
        "and Gru that return sequences and states and attention layer \n",
        "\n",
        "### enforcing learning \n",
        "we will use this teqniuque to teach the model and this basicly instead of using the ouput word of the decoder as next state input we use the right word from our target batch data "
      ],
      "metadata": {
        "id": "fPnItop9RWKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model): \n",
        "  def __init__(self  , embedding_dim , units,vocab_size , batch_size ) : \n",
        "    super(Decoder , self).__init__()\n",
        "    self.embedding_dim = embedding_dim \n",
        "    self.units = units \n",
        "    self.batch_size = batch_size\n",
        "    self.vocab_size = vocab_size\n",
        "    self.decoder_embedding = tf.keras.layers.Embedding(vocab_size , embedding_dim)\n",
        "    self.decoder_gru = tf.keras.layers.GRU(units , return_sequences=True ,return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "    self.attention = BahdanauAttention(units)\n",
        "    self.fc= tf.keras.layers.Dense(vocab_size) \n",
        "  def call(self, x, hidden, encoder_output ) : \n",
        "    # embedd the inputs \n",
        "    x = self.decoder_embedding(x) \n",
        "    # return the context and attention weights \n",
        "    context , attention_weights = self.attention(hidden , encoder_output) \n",
        "    # we concat the context vector with the previous right target word \n",
        "    x= tf.concat([tf.expand_dims( context, 1), x], \n",
        "                                      axis=-1)\n",
        "    output , state = self.decoder_gru(x )\n",
        "    output= tf.reshape(output, (-1, output.shape[2]))\n",
        "    x= self.fc(output)\n",
        "    return x , state , attention_weights \n"
      ],
      "metadata": {
        "id": "7OqD0xYtclVf"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder= Decoder( embedding_dim, units,target_vocab_size, BATCH_SIZE)\n",
        "sample_decoder_output, _, _= decoder(tf.random.uniform((BATCH_SIZE,1)), sample_hidden, sample_output)\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V_4SMPMBXY-",
        "outputId": "7e7d0cb3-e414-4d66-ca5c-665efe9faa96"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 13556)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "metadata": {
        "id": "gSZwD-VSjuTo"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training \n",
        "we will use the gradient tape to control the training process \n"
      ],
      "metadata": {
        "id": "OnbDGEC6TNgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(inp, targ, encoder_hidden): \n",
        "  loss = 0 \n",
        "  with tf.GradientTape() as tape : \n",
        "    # get output and the hidden states of the encoder \n",
        "    encoder_output , encoder_hidden = encoder(inp , encoder_hidden)\n",
        "    decoder_hidden = encoder_hidden\n",
        "    # we will assign first input to [start] tocken \n",
        "    decoder_input = tf.expand_dims(\n",
        "            [target_sentence_tokenizer.word_index['start_']] * BATCH_SIZE, 1)\n",
        "    # looping throw the lenth of the target words \n",
        "    for t in range(1, targ.shape[1]):\n",
        "          # passing encoder_output to the decoder\n",
        "          predictions, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_hidden)\n",
        "          loss += tf.keras.losses.sparse_categorical_crossentropy(targ[:, t], predictions)\n",
        "          # assign the new input as the right word from out target\n",
        "          decoder_input = tf.expand_dims(targ[:, t], 1)\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  # take the gradients\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return batch_loss"
      ],
      "metadata": {
        "id": "RRx63sqbiLBC"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS=20\n",
        "for epoch in range(EPOCHS):\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  # train the model using data in bataches \n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "  if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {}'.format(epoch + 1,\n",
        "                                                   batch,                                                   \n",
        "                                         batch_loss.numpy()))\n",
        "      print('Epoch {} Loss {}'.format(epoch + 1,total_loss / steps_per_epoch))"
      ],
      "metadata": {
        "id": "R-cOA69HlXg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_target_length= max(len(t) for t in  target_tensor)\n",
        "max_input_length= max(len(t) for t in input_tensor)"
      ],
      "metadata": {
        "id": "ELag6q6eC69J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluation \n",
        "you will predict but now we don,t know the rights target word so we will pass the output of each time stamp as new input "
      ],
      "metadata": {
        "id": "CaWTwDoxUkjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot= np.zeros((max_target_length, max_input_length))\n",
        "    #preprocess the sentnece\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    \n",
        "    #convert the sentence to index based on word2index dictionary\n",
        "    inputs= [input_sentence_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    \n",
        "    # pad the sequence \n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_input_length, padding='post')\n",
        "    \n",
        "    #conver to tensors\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    \n",
        "    result= ''\n",
        "    \n",
        "    # creating encoder\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    encoder_output, encoder_hidden= encoder(inputs, hidden)\n",
        "    \n",
        "    # creating decoder\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']], 0)\n",
        "    \n",
        "    for t in range(max_target_length):\n",
        "        predictions, decoder_hidden, attention_weights= decoder(decoder_input, decoder_hidden, encoder_output)\n",
        "        \n",
        "        # storing attention weight for plotting it\n",
        "        attention_weights = tf.reshape(attention_weights, (-1,))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "        \n",
        "        prediction_id= tf.argmax(predictions[0]).numpy()\n",
        "        result += target_sentence_tokenizer.index_word[prediction_id] + ' '\n",
        "        \n",
        "        if target_sentence_tokenizer.index_word[prediction_id] == '_end':\n",
        "            return result,sentence, attention_plot\n",
        "        \n",
        "        # predicted id is fed back to as input to the decoder\n",
        "        decoder_input = tf.expand_dims([prediction_id], 0)\n",
        "        \n",
        "    return result,sentence, attention_plot"
      ],
      "metadata": {
        "id": "QoIMmEKAlr5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "72HUVEu8EE9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "  \n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "metadata": {
        "id": "hHXqOhs9EJBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(u'we dit it .')"
      ],
      "metadata": {
        "id": "CmsGzepUK5Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## final word \n",
        "after going throw the whole model you should reread it again backword to beter understanding of the ambiguous thinks you faced "
      ],
      "metadata": {
        "id": "eQMn8QajVJ27"
      }
    }
  ]
}