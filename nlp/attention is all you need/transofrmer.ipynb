{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transofrmer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Attention is all you need \n",
        "the authors proposed a model which elminate recurrence and relying on the attention mechanism to draw global dependencies between input and output this lead to better preformance and more parallelization computing "
      ],
      "metadata": {
        "id": "43VMl6g2xHxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "lBG86vkmjvNX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# positional encoding \n",
        "we need this to compensate the absence of recurrence and to get use of the order of tokens we have to inject some information about the postion of the words so we use the sin and cos functions "
      ],
      "metadata": {
        "id": "RP95_hE1yf_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class positional_encoding : \n",
        "  def __init__(self) : \n",
        "    pass\n",
        "  def get_angles(self , pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "  def encoding(self , position, d_model):\n",
        "    angle_rads = self.get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "KsBVNVIJs5t6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# masking \n",
        "we will use two masks \n",
        "- first is padding mask to align the whole sequece length \n",
        "- second look ahead mask which help us with enforcing learning while computing in parallel this done by hashing part of the sentence which is the target for example ('I want orange ----') we hashed the word juice and it will be the target we will output "
      ],
      "metadata": {
        "id": "LFd9jATMzM6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class masking: \n",
        "  def __init__(self) : \n",
        "    pass\n",
        "  def create_look_ahead_mask( self,size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  \n",
        "  def create_padding_mask(self, seq) : \n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "metadata": {
        "id": "RaX8M1L3uGJn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# scaled dot product attetnion (self head attention) \n",
        "this the main part of the transformer which we basicly get (q, k ,v ) \n",
        "which are weighted paramters from the input and for each word we compute attention vector we compute that by multiply (q , k ) and pass the output to softmax function and multiply the output by (v) this give you good represention of each word \n",
        "\n",
        "A(q, k , v) = softmax(q*k)v"
      ],
      "metadata": {
        "id": "GGhLfwJV0cpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class self_head_attention(tf.keras.layers.Layer) : \n",
        "  def __init__(self ) : \n",
        "    super(self_head_attention , self).__init__()\n",
        "  def call(self ,q , k , v , mask ) : \n",
        "    qk_matmul = tf.matmul(q , k , transpose_b=True) \n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = qk_matmul / tf.math.sqrt(dk)\n",
        "    if mask is not None:\n",
        "      scaled_attention_logits += (mask * -1e9)\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "DuroVaDykhBh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# multi head attention \n",
        "It,s basicly repeating the self head attention to get different representions for the data \n",
        "every head is self head attention and you compute the heads in parallel and you concat them and multiply the ouput with W \n"
      ],
      "metadata": {
        "id": "c9_Q1mHx1plT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model , num_heads): \n",
        "    super(MultiHeadAttention , self ).__init__()\n",
        "    self.d_model = d_model \n",
        "    self.num_heads = num_heads \n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    # the layers to initiaize the q k v vectors \n",
        "    self.qw = tf.keras.layers.Dense(d_model)\n",
        "    self.kw = tf.keras.layers.Dense(d_model) \n",
        "    self.vw = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.head_attention = self_head_attention()\n",
        "    self.concat_weights = tf.keras.layers.Dense(d_model)\n",
        "  def split_heads(self , x , batch_size ) : \n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "  def call(self , v ,q ,k , mask ) : \n",
        "    batch_size = tf.shape(q)[0]\n",
        "    q = self.qw(q)   # (batch_size, seq_len, d_model)\n",
        "    k = self.kw(k)   # (batch_size, seq_len, d_model)\n",
        "    v = self.vw(v)   # (batch_size, seq_len, d_model)\n",
        "\n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "    scaled_attention, attention_weights = self.head_attention(\n",
        "        q, k, v, mask)\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "    output = self.concat_weights(concat_attention)\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "KaJ6tnFTmfOR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# feed forword \n",
        "it just basic block to pass the data throw nural networks "
      ],
      "metadata": {
        "id": "cpsNuPb93O_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class feed_forword_block(tf.keras.layers.Layer) :  \n",
        "  def __init__(self, d_model , dff) : \n",
        "    super(feed_forword_block , self).__init__()\n",
        "    self.dense_1 =  tf.keras.layers.Dense(dff, activation='relu') # (batch_size, seq_len, dff)\n",
        "    self.dense_2 =  tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  def call(self , X ) : \n",
        "    X = self.dense_1(X)\n",
        "    return self.dense_2(X)"
      ],
      "metadata": {
        "id": "LTd2i01tq6Oc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder layer\n",
        "the encoder layer contain \n",
        "- multi head attention \n",
        "- dropout layers \n",
        "- feed forword \n",
        "- normalization layer \n",
        "\n",
        "the data passed throw the multi head attention after that to the feed forword network and between there are dropout and normalization layer which also add skip connections to drive more information and keep track of the context of the word \n",
        "\n"
      ],
      "metadata": {
        "id": "annJZK5w3bQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer): \n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1)  : \n",
        "    super(EncoderLayer , self).__init__()\n",
        "    self.multi_head = MultiHeadAttention(d_model ,num_heads) \n",
        "    self.feed_forword = feed_forword_block(d_model , dff) \n",
        "\n",
        "    self.layernorm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout_1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout_2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self , x , training, mask ) : \n",
        "    atte_out , _ = self.multi_head(x,x,x ,mask)\n",
        "    atte_out = self.dropout_1(atte_out , training = training)\n",
        "    atte_out = self.layernorm_1(  atte_out +x )\n",
        "    feed_out = self.feed_forword(atte_out)\n",
        "    feed_out = self.dropout_2(feed_out , training = training )\n",
        "    return self.layernorm_2(x + feed_out)"
      ],
      "metadata": {
        "id": "SeNmVkgoruUa"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder \n",
        "the encoder contain \n",
        "- embedding layer \n",
        "- positional encoding \n",
        "- num times of the encoder layer block \n"
      ],
      "metadata": {
        "id": "_z_BAi4J4ZgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer): \n",
        "  def __init__(self , num_layers  , d_model , num_heads , dff ,input_vocab_size, maximum_position_encoding , rate = .1): \n",
        "      super(Encoder , self ).__init__()\n",
        "      self.d_model = d_model\n",
        "      self.maximum_position_encoding = maximum_position_encoding\n",
        "      self.num_layers = num_layers\n",
        "      self.pos = positional_encoding()\n",
        "      self.embed = tf.keras.layers.Embedding(input_vocab_size , d_model ) \n",
        "      self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                       for _ in range(num_layers)]\n",
        "\n",
        "      self.dropout = tf.keras.layers.Dropout(rate)\n",
        "  def call(self , x  , training ,mask) : \n",
        "    pos_encoding =self.pos.encoding(self.maximum_position_encoding, self.d_model)\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    x = self.embed(x) # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x  # (batch_size, input_seq_len, d_model)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ov1Bi1CWyaYT"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder \n",
        "the decoder contain \n",
        "- two multi head attention layers \n",
        "- feed forword network \n",
        "- dropout \n",
        "- normalization layers \n",
        "\n",
        "we pass the word throw the first multi head attention and after that we pass throw the second multi head attention the Q from the first multi head and (k , v) from the encoder output you can think about it as passing the qustion from the first multi head and getting the answers from the encoder after that we pass the ouput throw the feed forword network "
      ],
      "metadata": {
        "id": "e_SH3RVu40CD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer) : \n",
        "  def __init__(self , d_model , num_heads , dff  ,rate) : \n",
        "    super(DecoderLayer , self).__init__() \n",
        "    self.multi_head_1 = MultiHeadAttention(d_model , num_heads)\n",
        "    self.multi_head_2 = MultiHeadAttention(d_model , num_heads)\n",
        "    self.feed_forword = feed_forword_block(d_model , dff)\n",
        "    self.layernorm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.dropout_1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout_2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout_3 = tf.keras.layers.Dropout(rate)\n",
        "  def call(self , x , encoder_output , training , look_ahead_mask , padding_mask) : \n",
        "    X_1 , attn_weights_block1= self.multi_head_1(x ,x ,x , look_ahead_mask)\n",
        "    X_1 = self.dropout_1(X_1 , training = training) \n",
        "    X_1 = self.layernorm_1(x + X_1)\n",
        "\n",
        "    X_2 ,attn_weights_block2 = self.multi_head_2(q = X_1 ,k =  encoder_output , v = encoder_output , mask =padding_mask) \n",
        "    X_2 = self.dropout_2(X_2 , training = training )\n",
        "    X_2 = self.layernorm_2(x+ X_2) \n",
        "    X_3 = self.feed_forword(X_2)\n",
        "    X_3 = self.dropout_3(X_3 , training = training)\n",
        "    X_3 =  self.layernorm_3(x+X_3)\n",
        "    return X_3, attn_weights_block1, attn_weights_block2"
      ],
      "metadata": {
        "id": "oOAoCnrJt3AQ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer) : \n",
        "  def __init__(self , num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1) :\n",
        "    super(Decoder , self ).__init__()\n",
        "    self.d_model = d_model \n",
        "    self.maximum_position_encoding = maximum_position_encoding\n",
        "    self.num_layers = num_layers\n",
        "    self.embed = tf.keras.layers.Embedding(target_vocab_size  ,d_model)\n",
        "    self.pos= positional_encoding()\n",
        "    self.dec_layers = [DecoderLayer(d_model , num_heads , dff ,rate) for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self , x ,enc_output, training , look_ahead_mask, padding_mask ) :\n",
        "    pos_encoding = self.pos.encoding(self.maximum_position_encoding, self.d_model)\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "    x= self.embed(x) \n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights \n"
      ],
      "metadata": {
        "id": "C_jyg9Vv7EAF"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer \n",
        "this the whole model which we gather the encoder and decoder \n",
        "first we create the masks which we will need \n",
        "next we pass the data throw the model "
      ],
      "metadata": {
        "id": "BauM1VrF6Ode"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model) : \n",
        "  def __init__(self, n_layers , d_model  ,num_heads , dff , input_vocab_size , target_vocab_size , pe_input, pe_target, rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(n_layers , d_model  ,num_heads , dff , input_vocab_size  , rate )\n",
        "    self.decoder = Decoder(n_layers , d_model  ,num_heads , dff , target_vocab_size  , rate)\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size) \n",
        "    self.masking = masking()\n",
        "  def create_masks(self, inp , targ) : \n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = self.masking.create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = self.masking.create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by\n",
        "    # the decoder.\n",
        "    look_ahead_mask = self.masking.create_look_ahead_mask(tf.shape(targ)[1])\n",
        "    dec_target_padding_mask = self.masking.create_padding_mask(targ)\n",
        "    look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, look_ahead_mask, dec_padding_mask \n",
        "  def call(self , inputs ,  training  ) : \n",
        "    inp, targ = inputs\n",
        "    enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, targ)\n",
        "    enc_output = self.encoder(inp , training = training , mask = enc_padding_mask)\n",
        "    dec_output , attention_weights =  self.decoder(targ , enc_output  , training = training , padding_mask = dec_padding_mask , look_ahead_mask = look_ahead_mask)\n",
        "    out = self.final_layer(dec_output)\n",
        "    return out , attention_weights"
      ],
      "metadata": {
        "id": "Hcr4xwoR85LR"
      },
      "execution_count": 55,
      "outputs": []
    }
  ]
}